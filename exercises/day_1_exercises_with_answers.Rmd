---
title: "Day 1: Shrinkage Methods--Lasso and Ridge Regression (Exercises with Answers)"
author: "Emilio Lehoucq"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: true
    toc_float: true
    toc_depth: 3
params:
  notes: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.height = 4.35, fig.width = 4.75, message = FALSE, warning = FALSE)
```

# Question

Fit a kitchen sink OLS model and tune Lasso and Ridge models through 10-fold cross validation to predict `order` in the [clickstream data for online shopping Data Set](https://archive.ics.uci.edu/ml/datasets/clickstream+data+for+online+shopping). Which model fits the data best? Use a validation set approach.

# Answer

## Packages

```{r}
library(tidyverse)
library(janitor) # clean_names
library(robustHD) # standardize
library(skimr) # descriptive statistics
library(ggcorrplot) # correlation matrix
library(glmnet) # shrinkage methods
library(glmnetUtils) # improves glmnet
```

## Data

```{r}
set.seed(1) 

factors <- c("country", "page_1_main_category", "colour", 
             "location", "model_photography", "price_2", "page")

eshop <- read_delim(here::here("data/e-shop clothing 2008.csv"), delim = ";") %>% 
  sample_n(1500) %>% 
  clean_names() %>% 
  select(-c(`session_id`, `page_2_clothing_model`, year)) %>% 
  mutate_each_(funs(factor(.)), factors) %>% 
  mutate(log_order = log(order)) %>% 
  select(-order) %>% 
  mutate_if(is.numeric, funs(standardize(.))) %>% 
  select(-country) # otherwise there's problems with the test data. there are better ways to deal with this

```

```{r}
set.seed(1)

eshop_train <- eshop %>% sample_frac(0.7)
eshop_test <- eshop %>% setdiff(eshop_train)
```

## Exploratory data analysis

```{r}
skim(eshop_train)
```

```{r}
corr <- round(cor(eshop_train %>% select_if(is.numeric)), 1)

ggcorrplot(corr, lab = FALSE)
```

## OLS regression

```{r}
kitchen_sink <- lm(log_order ~ ., eshop_train)
  
summary(kitchen_sink)
```

## Ridge regression

```{r}
set.seed(1)

lambda_grid <- 10^seq(-2, 10, length = 200)

ridge_cv <- cv.glmnet(formula = log_order ~ ., 
                      data = eshop_train, 
                      alpha = 0, # Ridge!
                      nfolds = 10,
                      lambda = lambda_grid
    )
```

```{r}
plot(ridge_cv)
```

```{r}
ridge_lambda_min <- ridge_cv$lambda.min
ridge_lambda_1se <- ridge_cv$lambda.1se

ridge_min <- glmnet(log_order ~ ., data = eshop_train, alpha = 0, lambda = ridge_lambda_min)
ridge_1se <- glmnet(log_order ~ ., data = eshop_train, alpha = 0, lambda = ridge_lambda_1se)

coef(ridge_min)
```

```{r}
cbind(ridge_min$dev.ratio, ridge_1se$dev.ratio)
```

## Lasso regression

```{r}
set.seed(1)

lasso_cv <- cv.glmnet(formula = log_order ~ ., 
                      data = eshop_train, 
                      alpha = 1, # Lasso! 
                      nfolds = 10,
                      lambda = lambda_grid
    )
```

```{r}
plot(lasso_cv)
```

```{r}
lasso_lambda_min <- lasso_cv$lambda.min
lasso_lambda_1se <- lasso_cv$lambda.1se

lasso_min <- glmnet(log_order ~ ., data = eshop_train, alpha = 1, lambda = lasso_lambda_min)
lasso_1se <- glmnet(log_order ~ ., data = eshop_train, alpha = 1, lambda = lasso_lambda_1se)

coef(lasso_min)
```

```{r}
cbind(lasso_min$dev.ratio, lasso_1se$dev.ratio)
```

## Model validation

```{r}
# helper function
#' @name r2
#' @param model nnet object
#' @param test test data
#' @param outcome string with the response
#' @returns test R2
r2<- function(model, test, outcome){
  
  preds <- predict(model, test)
  
  sse <- sum((test[[outcome]] - preds)^2)
  sst <- sum((test[[outcome]] - mean(test[[outcome]]))^2)
  r2 <- 1 - sse / sst
  
  return(r2)
  
}
```

### OLS regression

```{r}
kitchen_sink_test <- lm(log_order ~ ., eshop_train)
  
r2(kitchen_sink_test, eshop_test, "log_order")
```

### Ridge regression

```{r}
ridge_min_test <- glmnet(log_order ~ ., data = eshop_train, alpha = 0, lambda = ridge_lambda_min)
ridge_1se_test <- glmnet(log_order ~ ., data = eshop_train, alpha = 0, lambda = ridge_lambda_1se)

cbind(r2(ridge_min_test, eshop_test, "log_order"), r2(ridge_1se_test, eshop_test, "log_order"))
```

### Lasso regression

```{r}
lasso_min_test <- glmnet(log_order ~ ., data = eshop_train, alpha = 1, lambda = lasso_lambda_min)
lasso_1se_test <- glmnet(log_order ~ ., data = eshop_train, alpha = 1, lambda = lasso_lambda_1se)

cbind(r2(lasso_min_test, eshop_test, "log_order"), r2(lasso_1se_test, eshop_test, "log_order"))
```

They all perform pretty much the same. Also, they all perform slightly worse on the test data.
