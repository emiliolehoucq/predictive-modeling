---
title: "Day 2: Tree-based methods--Random forests and boosted trees (Exercises with answers)"
author: "Emilio Lehoucq"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: true
    toc_float: true
    toc_depth: 3
params:
  notes: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.height = 4.35, fig.width = 4.75, message = FALSE, warning = FALSE)
```

# Question

Fit boosted trees and random forests through a validation set approach to predict `order` in the clickstream data for online shopping Data Set. Which model fits the data best? Use a validation set approach. Also, visualize the importance of predictors for each model and the marginal relation between the most important predictor and the response for the random forest.

# Answer

## Packages

```{r}
library(tidyverse)
library(janitor) # clean_names
library(ranger) # random forests
library(vip) # variable importance plot
library(pdp) # partial dependence plot
library(xgboost) # boosted trees
```

## Data

```{r}
set.seed(1)

factors <- c("country", "page_1_main_category", "colour", 
             "location", "model_photography", "price_2", "page")

eshop <- read_delim(here::here("data/e-shop clothing 2008.csv"), delim = ";") %>% 
  sample_n(1500) %>% 
  clean_names() %>% 
  select(-c(`session_id`, `page_2_clothing_model`, year)) %>% 
  mutate_each_(funs(factor(.)), factors) 
```

We're going to split the dataset into two for tuning and validation (and divide the tuning data into two for a validation set approach). This is not the best approach for these data, but we're doing it for instructional purposes.

```{r}
set.seed(1)

eshop_tune <- eshop %>% sample_frac(0.7)
eshop_val <- eshop %>%  setdiff(eshop_tune)

eshop_split <- tibble(train = eshop_tune %>% sample_frac(0.7) %>% list(),
                     test = eshop_tune %>% setdiff(train) %>% list())
```

# Random forests

Let's tune the number of variables to split at each node first through a validation set approach:

```{r}
set.seed(1)

random_forest <- eshop_split %>% 
  crossing(mtry = 1:(ncol(eshop) - 1)) %>% 
  mutate(model = map2(.x = train, .y = mtry, 
                      .f = function(x, y) ranger(order ~ ., 
                                                 mtry = y, 
                                                 data = x, 
                                                 splitrule = "variance",
                                                 importance = "impurity")), 
         oob_err = map(.x = model, .f = function(x) x[["prediction.error"]])
         )
```

Let's check we're tuning correctly:

```{r}
ggplot(random_forest) + 
  geom_line(aes(mtry, unlist(oob_err))) +
  labs(x = "mtry", y = "MSE")
```

Yes, the OOB MSE is at a minimum and keeps increases.

```{r}
random_forest %>% 
  arrange(unlist(oob_err)) %>% 
  pluck("mtry", 1)
```

The "best" number of variables to split at each node is 1.

You can also tune the maximum depth of each tree (max.depth) and the number of trees (num.trees). We'll keep it to the number of variables to split at each node for speed, and since it's the one that tends to have a higher impact.

## Variable importance plot

Let's do a variable importance plot.

```{r}
random_forest_best <- ranger(order ~ ., 
                      data = eshop_tune, 
                      mtry = 1,
                      importance = "impurity", 
                      splitrule = "variance")

vip(random_forest_best)
```

This is really useful.

## Partial dependence plot

Let's look at the marginal relation between particular predictors and the response.

```{r}
partial(random_forest_best, 
        pred.var = "day",
        pred.data = eshop_tune,
        plot = TRUE,
        rug = TRUE,
        plot.engine = "ggplot2") + 
  labs(y = "Order", x = "Day")
```

There seems to be a non-linear relation, which would be harder to model with linear models. However, we don't have much data. We run the risk of overfitting.

# Boosted trees

First, we'll need helper functions (adapted from Arend Kuyper's Data Science Manual):

```{r}
# helper function
#' @name xgb_matrix
#' @param dat tibble, dataset
#' @param outcome string, indicates the outcome variable in the data
#' @returns MSE of the model on the test set
xgb_matrix <- function(dat, outcome){

  # Sanitize input: check that dat is a tibble
  if(!is_tibble(dat)){
    
    dat <- as_tibble(dat)
    
  }
  
  # Sanitize input: check that data has factors, not characters
  dat_types <- dat %>% map_chr(class)
  
  outcome_type <- class(dat[[outcome]])
  
  if("character" %in% dat_types){
    
    # If we need to re-code, leave that outside of the function
    print("You must encode characters as factors.")
    return(NULL)
    
  } else {
  
    # If we're doing binary outcomes, they need to be 0-1
    if(outcome_type == "factor" & nlevels(dat[[outcome]]) == 2){
      tmp <- dat %>% select(outcome) %>% onehot::onehot() %>% predict(dat)  
      lab <- tmp[,1]
    } else {
      lab <- dat[[outcome]]
    }
    
    # Make our DMatrix
    mat <- dat %>% dplyr::select(-outcome) %>% # encode on full boston df
      onehot::onehot() %>% # use onehot to encode variables
      predict(dat) # get OHE matrix

    return(xgb.DMatrix(data = mat, 
                       label = lab))
    
  }
  
}

# helper function
#' @name xg_error
#' @param model xgb object, a fitted boosted model
#' @param test_mat DMatrix, a test set
#' @param metric string (either "r2" or "misclass"), indicates the error metric
#' @returns MSE/misclass rate of the model on the test set
xg_error <- function(model, test_mat, metric = "r2"){
  
  # Get predictions and actual values
  preds = predict(model, test_mat)
  vals = getinfo(test_mat, "label")
  
  if(metric == "r2"){
    
    # Compute R2 if that's what we need
    sse <- sum((vals - preds)^2)
    sst <- sum((vals - mean(vals))^2)
    err <- 1 - sse / sst
    
  } else if(metric == "misclass") {
    
    # Otherwise, get the misclass rate
    err <- mean(preds != vals)
    
  }
  
  return(err)
}
```

Again, we're tuning boosted trees through a validation set approach. Let's tune the learning rate:

```{r}
set.seed(1)

boosted_tree <- eshop_split %>%
   crossing(learn_rate = 10^seq(-10, -0.1, length.out = 20)) %>% 
   mutate(train_mat = map(train, xgb_matrix, outcome = "order"), 
          test_mat = map(test, xgb_matrix, outcome = "order"),
          xg_model = map2(.x = train_mat, .y = learn_rate, 
                          .f = function(x, y) xgb.train(params = list(eta = y,
                                                                      depth = 10, 
                                                                      objective = "reg:squarederror"), 
                                                        data = x, 
                                                        nrounds = 500,
                                                        silent = TRUE)), 
          xg_test_mse = map2(xg_model, test_mat, xg_error, metric = "r2"))
```

Let's check that we're tuning correctly:

```{r}
ggplot(boosted_tree) + 
  geom_line(aes(learn_rate, unlist(xg_test_mse))) +
  labs(x = "Learning Rate", y = "R2")
```

Yes, the test $R^2$ increases to 0 and then decreases further away from 0. This clearly tells us we're overfitting our data and doing a worse job than predicting the mean.

```{r}
boosted_tree %>% 
  arrange(abs(unlist(xg_test_mse))) %>%
  pluck("learn_rate", 1) 
```

The "best" learning rate is 0.001971228

Let's also tune the number of trees fit. It's better to tune all parameters at the same time. This obviously requires more computational capacity. You can do it with map functions, but would need more time and computing power. Here we're taking a "greedy" approach to tuning.

```{r}
set.seed(1)

boosted_tree <- eshop_split %>%
   crossing(num_trees = seq(from = 400, to = 700, by = 10)) %>% 
   mutate(train_mat = map(train, xgb_matrix, outcome = "order"), 
          test_mat = map(test, xgb_matrix, outcome = "order"),
          xg_model = map2(.x = train_mat, .y = num_trees, 
                          .f = function(x, y) xgb.train(params = list(eta = 0.001971228,
                                                                      depth = 10, 
                                                                      objective = "reg:squarederror"), 
                                                        data = x, 
                                                        nrounds = y,
                                                        silent = TRUE)), 
          xg_test_mse = map2(xg_model, test_mat, xg_error, metric = "r2"))
```

Let's check that we're tunning correctly:

```{r}
ggplot(boosted_tree) + 
  geom_line(aes(num_trees, unlist(xg_test_mse))) +
  labs(x = "Number of trees fit", y = "R2")
```

Again, the test $R^2$ increases to 0 and then decreases further away from 0. This again tells us we're overfitting our data and doing a worse job than predicting the mean.

```{r}
boosted_tree %>% 
  arrange(abs(unlist(xg_test_mse))) %>%
  pluck("num_trees", 1) # 600
```

The "best" number of trees fit is 600.

You can also tune `gamma`, `max_depth`, `min_child_weight`, and `colsample_bytree` as well. For speed--and because they usually have the largest impact, we'll only tune the learning rate and the number of trees fit.

## Variable importance plot

We can also look at variable importance:

```{r}
boosted_tree_best <- boosted_tree %>% 
  arrange(unlist(xg_test_mse)) %>%
  pluck("xg_model", 1)

vip(boosted_tree_best)
```

# Model validation

## Random forest

Helper function (adjusted from Arend Kuyper's Data Science Manual):

```{r}
# helper function to get misclass rate from a ranger object
#' @name r2_ranger
#' @param model ranger object, a fitted random forest
#' @param test tibble/resample object, a test set
#' @param outcome string, indicates the outcome variable in the data
#' @returns r2 of the model on the test set
r2_ranger <- function(model, test, outcome){
  
  # Check if test is a tibble
  if(!is_tibble(test)){
    test <- test %>% as_tibble()
  }
  
  # Make predictions
  preds <- predict(model, test)$predictions
  
  # Compute R2
  sse <- sum((test[[outcome]] - preds)^2)
  sst <- sum((test[[outcome]] - mean(test[[outcome]]))^2)
  r2 <- 1 - sse / sst
  
  return(r2)
  
}
```

```{r}
random_forest_best_test <- ranger(order ~ ., 
                      data = eshop_tune, 
                      mtry = 1,
                      importance = "impurity", 
                      splitrule = "variance")

r2_ranger(random_forest_best_test, eshop_val, "order")
```

## Boosted tree

```{r}
boosted_tree_best_test <- xgb.train(params = list(eta = 0.001971228,
                        depth = 10, 
                        objective = "reg:squarederror"),
          data = xgb_matrix(eshop_tune, "order"), 
          nrounds = 600,
          silent = TRUE)

xg_error(boosted_tree_best_test, xgb_matrix(eshop_val, "order"), metric = "r2")
```

Neither of them is performing great. We're definitely overfitting our data.

Remember the $R^2s$ from yesterday:

| Model            | Test R2 |
|------------------|---------|
| Kitchen sink OLS | 0.17    |
| Ridge            | 0.14    |
| Lasso            | 0.13    |
| Random forests   | 0.03    |
| Boosted trees    | -0.06   |

Linear models seem to perform best on these data. Keep in mind we're using very little data overall and taking even less advantage given our cross validation strategy. It'd be better to do k-fold cross validation. For instructional purposes (to cover both k-fold cross validation and the validation set approach) and for speed, we're using this strategy.
