---
title: "Day 2: Tree-based methods--Random forests and boosted trees (Exercises with answers)"
author: "Emilio Lehoucq"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: true
    toc_float: true
    toc_depth: 3
params:
  notes: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.height = 4.35, fig.width = 4.75, message = FALSE, warning = FALSE)
```

# Question

Fit boosted trees and random forests through a validation set approach to predict `order` in the [clickstream data for online shopping Data Set](https://archive.ics.uci.edu/ml/datasets/clickstream+data+for+online+shopping). Which model fits the data best? Use a validation set approach. Also, visualize the importance of predictors for each model and the marginal relation between the most important predictor and the response for the random forest.

# Answer

## Packages

```{r}
library(tidyverse)
library(janitor)
library(ranger)
library(vip)
library(pdp)
library(xgboost)
```

## Data

```{r}
set.seed(1)

factors <- c("country", "page_1_main_category", "colour", 
             "location", "model_photography", "price_2", "page")

eshop <- read_delim(here::here("data/e-shop clothing 2008.csv"), delim = ";") %>% 
  sample_n(1500) %>% 
  clean_names() %>% 
  select(-c(`session_id`, `page_2_clothing_model`, year)) %>% 
  mutate_each_(funs(factor(.)), factors) 
```

```{r}
set.seed(1)

eshop_tune <- eshop %>% sample_frac(0.7)
eshop_val <- eshop %>%  setdiff(eshop_tune)

train <- eshop_tune %>% sample_frac(0.7)
test <- eshop_tune %>% setdiff(train)

eshop_split <- tibble(train = train %>% list(),
                     test = test %>% list())
```

# Random forests

```{r}
set.seed(1)

random_forest <- eshop_split %>% 
  crossing(mtry = 1:(ncol(eshop) - 1)) %>% 
  mutate(model = map2(.x = train, .y = mtry, 
                      .f = function(x, y) ranger(order ~ ., 
                                                 mtry = y, 
                                                 data = x, 
                                                 splitrule = "variance",
                                                 importance = "impurity")), 
         oob_err = map(.x = model, .f = function(x) x[["prediction.error"]])
         )
```

```{r}
ggplot(random_forest) + 
  geom_line(aes(mtry, unlist(oob_err))) +
  labs(x = "mtry", y = "MSE")
```

```{r}
random_forest %>% 
  arrange(unlist(oob_err)) %>% 
  pluck("mtry", 1)
```

## Variable importance plot

```{r}
random_forest_best <- ranger(order ~ ., 
                      data = eshop_tune, 
                      mtry = 1,
                      importance = "impurity", 
                      splitrule = "variance")

vip(random_forest_best)
```

## Partial dependence plot

```{r}
partial(random_forest_best, 
        pred.var = "day",
        pred.data = eshop_tune,
        plot = TRUE,
        rug = TRUE,
        plot.engine = "ggplot2") + 
  labs(y = "Order", x = "Day")
```

# Boosted trees

```{r}
# Adapted from Arend Kuyper's Data Science Manual

# helper function
#' @name xgb_matrix
#' @param dat tibble, dataset
#' @param outcome string, indicates the outcome variable in the data
#' @returns xgb.Dmatrix object
xgb_matrix <- function(dat, outcome){

  # Sanitize input: check that data has factors, not characters
  dat_types <- dat %>% map_chr(class)
  
  outcome_type <- class(dat[[outcome]])
  
  if("character" %in% dat_types){
    
    # If we need to re-code, leave that outside of the function
    print("You must encode characters as factors.")
    return(NULL)
    
  } else {
  
    # If we're doing binary outcomes, they need to be 0-1
    if(outcome_type == "factor" & nlevels(dat[[outcome]]) == 2){
      tmp <- dat %>% select(outcome) %>% onehot::onehot() %>% predict(dat)  
      lab <- tmp[,1]
    } else {
      lab <- dat[[outcome]]
    }
    
    # Make our DMatrix
    mat <- dat %>% dplyr::select(-outcome) %>% # encode on full boston df
      onehot::onehot() %>% # use onehot to encode variables
      predict(dat) # get OHE matrix

    return(xgb.DMatrix(data = mat, 
                       label = lab))
    
  }
  
}

# helper function
#' @name xg_error
#' @param model xgb object, a fitted boosted model
#' @param test_mat DMatrix, a test set
#' @param metric string (either "r2" or "misclass"), indicates the error metric
#' @returns R2/misclass rate of the model on the test set
xg_error <- function(model, test_mat, metric = "r2"){
  
  # Get predictions and actual values
  preds = predict(model, test_mat)
  vals = getinfo(test_mat, "label")
  
  if(metric == "r2"){
    
    # Compute R2 if that's what we need
    sse <- sum((vals - preds)^2)
    sst <- sum((vals - mean(vals))^2)
    err <- 1 - sse / sst
    
  } else if(metric == "misclass") {
    
    # Otherwise, get the misclass rate
    err <- mean(preds != vals)
    
  }
  
  return(err)
}
```

```{r}
set.seed(1)

boosted_tree <- eshop_split %>%
   crossing(learn_rate = 10^seq(-10, -0.1, length.out = 20)) %>% 
   mutate(train_mat = map(train, xgb_matrix, outcome = "order"), 
          test_mat = map(test, xgb_matrix, outcome = "order"),
          xg_model = map2(.x = train_mat, .y = learn_rate, 
                          .f = function(x, y) xgb.train(params = list(eta = y,
                                                                      depth = 10, 
                                                                      objective = "reg:squarederror"), 
                                                        data = x, 
                                                        nrounds = 500,
                                                        silent = TRUE)), 
          xg_test_mse = map2(xg_model, test_mat, xg_error, metric = "r2"))
```

```{r}
ggplot(boosted_tree) + 
  geom_line(aes(learn_rate, unlist(xg_test_mse))) +
  labs(x = "Learning Rate", y = "R2")
```

```{r}
boosted_tree %>% 
  arrange(abs(unlist(xg_test_mse))) %>%
  pluck("learn_rate", 1) 
```

```{r}
set.seed(1)

boosted_tree <- eshop_split %>%
   crossing(num_trees = seq(from = 400, to = 700, by = 10)) %>% 
   mutate(train_mat = map(train, xgb_matrix, outcome = "order"), 
          test_mat = map(test, xgb_matrix, outcome = "order"),
          xg_model = map2(.x = train_mat, .y = num_trees, 
                          .f = function(x, y) xgb.train(params = list(eta = 0.001971228,
                                                                      depth = 10, 
                                                                      objective = "reg:squarederror"), 
                                                        data = x, 
                                                        nrounds = y,
                                                        silent = TRUE)), 
          xg_test_mse = map2(xg_model, test_mat, xg_error, metric = "r2"))
```

```{r}
ggplot(boosted_tree) + 
  geom_line(aes(num_trees, unlist(xg_test_mse))) +
  labs(x = "Number of trees fit", y = "R2")
```

```{r}
boosted_tree %>% 
  arrange(abs(unlist(xg_test_mse))) %>%
  pluck("num_trees", 1)
```

## Variable importance plot

```{r}
boosted_tree_best <- boosted_tree %>% 
  arrange(unlist(xg_test_mse)) %>%
  pluck("xg_model", 1)

vip(boosted_tree_best)
```

# Model validation

## Random forest

```{r}
# Adjusted from Arend Kuyper's Data Science Manual

# helper function to get error from a ranger object
#' @name r2_ranger
#' @param model ranger object, a fitted random forest
#' @param test tibble/resample object, a test set
#' @param outcome string, indicates the outcome variable in the data
#' @returns r2 of the model on the test set
r2_ranger <- function(model, test, outcome){
  
  # Make predictions
  preds <- predict(model, test)$predictions
  
  # Compute R2
  sse <- sum((test[[outcome]] - preds)^2)
  sst <- sum((test[[outcome]] - mean(test[[outcome]]))^2)
  r2 <- 1 - sse / sst
  
  return(r2)
  
}
```

```{r}
random_forest_best_test <- ranger(order ~ ., 
                      data = eshop_tune, 
                      mtry = 1,
                      importance = "impurity", 
                      splitrule = "variance")

r2_ranger(random_forest_best_test, eshop_val, "order")
```

## Boosted tree

```{r}
boosted_tree_best_test <- xgb.train(params = list(eta = 0.001971228,
                        depth = 10, 
                        objective = "reg:squarederror"),
          data = xgb_matrix(eshop_tune, "order"), 
          nrounds = 600,
          silent = TRUE)

xg_error(boosted_tree_best_test, xgb_matrix(eshop_val, "order"), metric = "r2")
```

| Model            | Test R2 |
|------------------|---------|
| Kitchen sink OLS | 0.08    |
| Ridge            | 0.08    |
| Lasso            | 0.09    |
| Random forests   | 0.03    |
| Boosted trees    | -0.06   |
