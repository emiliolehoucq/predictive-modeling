---
title: "Day 2: Tree-based methods--Random forests and boosted trees"
author: "Emilio Lehoucq"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: true
    toc_float: true
    toc_depth: 3
params:
  notes: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.height = 4.35, fig.width = 4.75, message = FALSE, warning = FALSE)
```

This session covers tree-based methods, specifically random forests and boosted trees. I'd also recommend you to take a look at Bayesian Additive Regression Trees. Decision trees recursively partition the predictor space and assign a constant in a terminal node. Trees have the advantage of performing variable selection. Random forests average across the predictions of trees grown on bootstrap samples of the dataset. Any single tree is constrained to be grown on a subset of predictors, which decorrelates the resulting trees and improves the accuracy of the final model. Boosted trees are grown sequentially on the same data. Each tree is grown using the residuals from the previous tree, which improves the fit.

# Packages

These are the packages we're going to use throughout the session:

```{r}
library(tidyverse)
library(janitor) # clean_names
library(ranger) # random forests
library(vip) # variable importance plot
library(pdp) # partial dependence plot
library(xgboost) # boosted trees
```

# Data

We're going to use the [Online News Popularity Data Set](https://archive.ics.uci.edu/ml/datasets/Online+News+Popularity) from the UCI Machine Learning Repository. The target is the number of shares (`shares`). Here I'm dealing with quick data wrangling to get the data ready for modeling.

```{r}
set.seed(1) 

factors <- c("data_channel_is_lifestyle", "data_channel_is_entertainment", "data_channel_is_bus", 
             "data_channel_is_socmed", "data_channel_is_tech", "data_channel_is_world", 
             "weekday_is_monday", "weekday_is_tuesday", "weekday_is_wednesday", 
             "weekday_is_thursday", "weekday_is_friday", "weekday_is_saturday",
             "weekday_is_sunday", "is_weekend") 

news <- read_csv(here::here("data/OnlineNewsPopularity.csv")) %>% 
  sample_n(., 1500) %>% 
  clean_names() %>% 
  select(-c(url, lda_00:lda_04)) %>% 
  mutate_each_(funs(factor(.)), factors) 
```

In statistical learning/predictive modeling, you don't care about how your model performs in the data you have at hand, but how it would perform either in the population or in new data that comes your way. Of course, you don't have these data. Cross validation is a set of resampling methods to address this problem. It allows you to estimate your test error. It's important to not use the same set of data for model tuning and for model validation. There are three main types of cross validation: validation set approach, leave one out cross validation, and k-fold cross validation. K-fold cross validation (where usually k=10, 5 or 3, depending on data/computing power) is the ideal. Also, since cross validation involves randomization, it'll give different results across iterations. Ideally, you can do several and average across them.

Today, we're using the validation set approach for both tuning and validation. Now, we're going to split the dataset into two for tuning and validation (and divide the tuning data into two for a validation set approach). This is not the best approach for these data, but we're doing it for instructional purposes.

```{r}
set.seed(1)

news_tune <- news %>% sample_frac(0.7)
news_val <- news %>%  setdiff(news_tune)

news_split <- tibble(train = news_tune %>% sample_frac(0.7) %>% list(),
                     test = news_tune %>% setdiff(train) %>% list())
```

# Random forests

Let's tune the number of variables to split at each node first through a validation set approach:

```{r}
set.seed(1)

random_forest <- news_split %>% 
  crossing(mtry = 1:(ncol(news) - 1)) %>% 
  mutate(model = map2(.x = train, .y = mtry, 
                      .f = function(x, y) ranger(shares ~ ., 
                                                 mtry = y, 
                                                 data = x, 
                                                 splitrule = "variance",
                                                 importance = "impurity")), 
         oob_err = map(.x = model, .f = function(x) x[["prediction.error"]])
         )
```

Let's check we're tuning correctly:

```{r}
ggplot(random_forest) + 
  geom_line(aes(mtry, unlist(oob_err))) +
  labs(x = "mtry", y = "MSE")
```

Yes, the OOB MSE decreases and then keeps increases. We've reached our minima.

```{r}
random_forest %>% 
  arrange(unlist(oob_err)) %>% 
  pluck("mtry", 1)
```

The "best" number of variables to split at each node is 3.

You can also tune the maximum depth of each tree (`max.depth`) and the number of trees (`num.trees`). We'll keep it to the number of variables to split at each node for speed, and since it's the one that tends to have a higher impact.

## Classification

For classification, you need to change `splitrule` to `gini`. Otherwise the code stays the same.

## Variable importance plot

Another nice feature of tree-based methods is that there is a natural measure of variable importance, which you can visualize through a variable importance plot. This is really useful to interpret the model.

```{r}
random_forest_best <- ranger(shares ~ ., 
                      data = news_tune, 
                      mtry = 3,
                      importance = "impurity", 
                      splitrule = "variance")

vip(random_forest_best)
```

In this case, the most important variable seems to be `num_hrefs`.

## Partial dependence plot

Interpreting a single tree, at least one with not many nodes, is very straighforward. Ensembles of trees are more black-boxed. However, you can graph the marginal relations between specific predictors and the response to visualize the model. Here, we'll do the partial dependence plot for `num_hrefs`.

```{r}
partial(random_forest_best, 
        pred.var = "num_hrefs",
        pred.data = news_tune,
        plot = TRUE,
        rug = TRUE,
        plot.engine = "ggplot2") + 
  labs(y = "Number of shares", x = "Number of links")
```

There seems to be a non-linear relation, which would be harder to model with linear models. However, we clearly don't have much data for most of the range of the predictor and even where there's data there's not a lot. We run the risk of overfitting.

# Boosted trees

Two helper functions to produce `xgb_matrix` and the test error:

```{r}
# Adapted from Arend Kuyper's Data Science Manual

# helper function
#' @name xgb_matrix
#' @param dat tibble, dataset
#' @param outcome string, indicates the outcome variable in the data
#' @returns xgb.Dmatrix object
xgb_matrix <- function(dat, outcome){

  # Sanitize input: check that data has factors, not characters
  dat_types <- dat %>% map_chr(class)
  
  outcome_type <- class(dat[[outcome]])
  
  if("character" %in% dat_types){
    
    # If we need to re-code, leave that outside of the function
    print("You must encode characters as factors.")
    return(NULL)
    
  } else {
  
    # If we're doing binary outcomes, they need to be 0-1
    if(outcome_type == "factor" & nlevels(dat[[outcome]]) == 2){
      tmp <- dat %>% select(outcome) %>% onehot::onehot() %>% predict(dat)  
      lab <- tmp[,1]
    } else {
      lab <- dat[[outcome]]
    }
    
    # Make our DMatrix
    mat <- dat %>% dplyr::select(-outcome) %>% # encode on full boston df
      onehot::onehot() %>% # use onehot to encode variables
      predict(dat) # get OHE matrix

    return(xgb.DMatrix(data = mat, 
                       label = lab))
    
  }
  
}

# helper function
#' @name xg_error
#' @param model xgb object, a fitted boosted model
#' @param test_mat DMatrix, a test set
#' @param metric string (either "r2" or "misclass"), indicates the error metric
#' @returns R2/misclass rate of the model on the test set
xg_error <- function(model, test_mat, metric = "r2"){
  
  # Get predictions and actual values
  preds = predict(model, test_mat)
  vals = getinfo(test_mat, "label")
  
  if(metric == "r2"){
    
    # Compute R2 if that's what we need
    sse <- sum((vals - preds)^2)
    sst <- sum((vals - mean(vals))^2)
    err <- 1 - sse / sst
    
  } else if(metric == "misclass") {
    
    # Otherwise, get the misclass rate
    err <- mean(preds != vals)
    
  }
  
  return(err)
}
```

The first hyperparameter we're tuning is the learning rate:

```{r}
set.seed(1)

boosted_tree <- news_split %>%
   crossing(learn_rate = 10^seq(-10, -0.1, length.out = 20)) %>% 
   mutate(train_mat = map(train, xgb_matrix, outcome = "shares"), 
          test_mat = map(test, xgb_matrix, outcome = "shares"),
          xg_model = map2(.x = train_mat, .y = learn_rate, 
                          .f = function(x, y) xgb.train(params = list(eta = y,
                                                                      depth = 10, 
                                                                      objective = "reg:squarederror"), 
                                                        data = x, 
                                                        nrounds = 500,
                                                        silent = TRUE)), 
          xg_test_mse = map2(xg_model, test_mat, xg_error, metric = "r2"))
```

Let's check that we're tuning correctly:

```{r}
ggplot(boosted_tree) + 
  geom_line(aes(learn_rate, unlist(xg_test_mse))) +
  labs(x = "Learning Rate", y = "R2")
```

Yes, the test $R^2$ increases to 0 and then decreases further away from 0. This clearly tells us we're overfitting our data and doing a worse job than predicting the mean.

```{r}
boosted_tree %>% 
  arrange(abs(unlist(xg_test_mse))) %>%
  pluck("learn_rate", 1)
```

The "best" learning rate is 0.0005938602.

Let's also tune the number of trees fit. It's better to tune all parameters at the same time. This obviously requires more computational capacity. You can do it with map functions (`pmap()` allows you to provide any number of arguments), but would need more time and computing power. Here we're taking a "greedy" approach to tuning.

```{r}
set.seed(1)

boosted_tree <- news_split %>%
   crossing(num_trees = seq(from = 600, to = 900, by = 10)) %>% 
   mutate(train_mat = map(train, xgb_matrix, outcome = "shares"), 
          test_mat = map(test, xgb_matrix, outcome = "shares"),
          xg_model = map2(.x = train_mat, .y = num_trees, 
                          .f = function(x, y) xgb.train(params = list(eta = 0.0005938602,
                                                                      depth = 10, 
                                                                      objective = "reg:squarederror"), 
                                                        data = x, 
                                                        nrounds = y,
                                                        silent = TRUE)), 
          xg_test_mse = map2(xg_model, test_mat, xg_error, metric = "r2"))
```

Let's check that we're tunning correctly:

```{r}
ggplot(boosted_tree) + 
  geom_line(aes(num_trees, unlist(xg_test_mse))) +
  labs(x = "Number of trees fit", y = "R2")
```

Again, the test $R^2$ increases to 0 and then decreases further away from 0. This tells us again that we're overfitting our data and doing a worse job than predicting the mean.

```{r}
boosted_tree %>% 
  arrange(abs(unlist(xg_test_mse))) %>%
  pluck("num_trees", 1)
```

The "best" number of trees fit is 770.

You can also tune `gamma`, `max_depth`, `min_child_weight`, and `colsample_bytree` as well. For speed--and because they usually have the largest impact--, we'll only tune the learning rate and the number of trees fit.

## Classification

For classification, you have to change `objective` to `"multi:softmax"` and set `num_class` to the number of classes (if binary, 2). You'd also have to change to `metric == "misclass"` in the `xg_error` function. Otherwise the code stays the same.

## Variable importance plot

We can also visualize variable importance for boosted trees:

```{r}
boosted_tree_best <- boosted_tree %>% 
  arrange(unlist(xg_test_mse)) %>%
  pluck("xg_model", 1)

vip(boosted_tree_best)
```

## Partial dependence plot

You can't use `partial()` to create partial dependence plots for an object of class `xgb.Booster`. However, you can also fit boosted trees with the package `gbm` and create partial dependence plots with `plot.gbm()`.

# Model validation

## Random forest

Helper function to calculate the $R^2$:

```{r}
# Adjusted from Arend Kuyper's Data Science Manual

# helper function to get error from a ranger object
#' @name r2_ranger
#' @param model ranger object, a fitted random forest
#' @param test tibble/resample object, a test set
#' @param outcome string, indicates the outcome variable in the data
#' @returns r2 of the model on the test set
r2_ranger <- function(model, test, outcome){
  
  # Make predictions
  preds <- predict(model, test)$predictions
  
  # Compute R2
  sse <- sum((test[[outcome]] - preds)^2)
  sst <- sum((test[[outcome]] - mean(test[[outcome]]))^2)
  r2 <- 1 - sse / sst
  
  return(r2)
  
}
```

```{r}
random_forest_best_test <- ranger(shares ~ ., 
                      data = news_tune, 
                      mtry = 3,
                      importance = "impurity", 
                      splitrule = "variance")

r2_ranger(random_forest_best_test, news_val, "shares")
```

## Boosted tree

```{r}
boosted_tree_best_test <- xgb.train(params = list(eta = 0.0005938602,
                        depth = 10, 
                        objective = "reg:squarederror"),
          data = xgb_matrix(news_tune, "shares"), 
          nrounds = 770,
          silent = TRUE)

xg_error(boosted_tree_best_test, xgb_matrix(news_val, "shares"), metric = "r2")
```

Neither of them is performing great. We're definitely overfitting our data.

Remember the $R^2s$ from yesterday:

| Model            | Test R2 |
|------------------|---------|
| Kitchen sink OLS | 0.11    |
| Ridge            | 0.10    |
| Lasso            | 0.10    |
| Random forests   | 0.03    |
| Boosted trees    | -0.06   |

Linear models seem to perform best on these data. Keep in mind we're using very little data overall and taking even less advantage given our cross validation strategy. 
