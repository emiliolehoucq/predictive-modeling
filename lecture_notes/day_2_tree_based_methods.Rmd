---
title: "Day 2: Tree-based methods--Random forests and boosted trees"
author: "Emilio Lehoucq"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: true
    toc_float: true
    toc_depth: 3
params:
  notes: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.height = 4.35, fig.width = 4.75, message = FALSE, warning = FALSE)
```

# Packages

```{r}
library(tidyverse)
library(janitor) # clean_names
library(skimr) # descriptive statistics
library(ggcorrplot) # correlation matrix
library(ranger) # random forests
library(vip) # variable importance plot
library(pdp) # partial dependence plot
library(xgboost) # boosted trees
```

# Data

```{r}
set.seed(1) # this is what we're going to use every time there is randomization

factors <- c("data_channel_is_lifestyle", "data_channel_is_entertainment", "data_channel_is_bus", 
             "data_channel_is_socmed", "data_channel_is_tech", "data_channel_is_world", 
             "weekday_is_monday", "weekday_is_tuesday", "weekday_is_wednesday", 
             "weekday_is_thursday", "weekday_is_friday", "weekday_is_saturday",
             "weekday_is_sunday", "is_weekend") # all factors in the dataset

news <- read_csv(here::here("data/OnlineNewsPopularity.csv")) %>% 
  sample_n(., 1500) %>% # we're only going to use 1,500 observations to run models faster during the sessions
  clean_names() %>% # to have all column names lower case letters
  select(-c(url, lda_00:lda_04)) %>% # we don't want this predictors
  mutate_each_(funs(factor(.)), factors) # converting to factors --check: sapply(news, class)
```

We're going to split the dataset into two for tuning and validation (and divide the tuning data into two for a validation set approach). This is not the best approach for these data, but we're doing it for instructional purposes.

```{r}
set.seed(1)

news_tune <- news %>% sample_frac(0.7)
news_val <- news %>%  setdiff(news_tune)

news_split <- tibble(train = news_tune %>% sample_frac(0.7) %>% list(),
                     test = news_tune %>% setdiff(train) %>% list())
```

# Random forests

Let's tune the number of variables to split at each node first through a validation set approach:

```{r}
set.seed(1)

random_forest <- news_split %>% 
  crossing(mtry = 1:(ncol(news) - 1)) %>% 
  mutate(model = map2(.x = train, .y = mtry, 
                      .f = function(x, y) ranger(shares ~ ., 
                                                 mtry = y, 
                                                 data = x, 
                                                 splitrule = "variance",
                                                 importance = "impurity")), 
         oob_err = map(.x = model, .f = function(x) x[["prediction.error"]])
         )
```

Let's check we're tuning correctly:

```{r}
ggplot(random_forest) + 
  geom_line(aes(mtry, unlist(oob_err))) +
  labs(x = "mtry", y = "MSE")
```

Yes, the OOB MSE decreases and then keeps increases. We've reached our minima.

```{r}
random_forest %>% 
  arrange(unlist(oob_err)) %>% 
  pluck("mtry", 1)
```

The "best" number of variables to split at each node is 3.

You can also tune the maximum depth of each tree (max.depth) and the number of trees (num.trees). We'll keep it to the number of variables to split at each node for speed, and since it's the one that tends to have a higher impact.

## Classification

For classification, you need to change `splitrule` to `gini`.

## Variable importance plot

Let's do a variable importance plot.

```{r}
random_forest_best <- ranger(shares ~ ., 
                      data = news_tune, 
                      mtry = 3,
                      importance = "impurity", 
                      splitrule = "variance")

vip(random_forest_best)
```

This is really useful.

## Partial dependence plot

Let's look at the marginal relation between particular predictors and the response.

```{r}
partial(random_forest_best, 
        pred.var = "num_hrefs",
        pred.data = news_tune,
        plot = TRUE,
        rug = TRUE,
        plot.engine = "ggplot2") + 
  labs(y = "Number of shares", x = "Number of links")
```

There seems to be a non-linear relation, which would be harder to model with linear models. However, we clearly don't have much data for most of the range of the predictor and even where there's data there's not a lot. We run the risk of overfitting.

# Boosted trees

First, we'll need helper functions (adapted from Arend Kuyper's Data Science Manual):

```{r}
# helper function
#' @name xgb_matrix
#' @param dat tibble, dataset
#' @param outcome string, indicates the outcome variable in the data
#' @returns MSE of the model on the test set
xgb_matrix <- function(dat, outcome){

  # Sanitize input: check that dat is a tibble
  if(!is_tibble(dat)){
    
    dat <- as_tibble(dat)
    
  }
  
  # Sanitize input: check that data has factors, not characters
  dat_types <- dat %>% map_chr(class)
  
  outcome_type <- class(dat[[outcome]])
  
  if("character" %in% dat_types){
    
    # If we need to re-code, leave that outside of the function
    print("You must encode characters as factors.")
    return(NULL)
    
  } else {
  
    # If we're doing binary outcomes, they need to be 0-1
    if(outcome_type == "factor" & nlevels(dat[[outcome]]) == 2){
      tmp <- dat %>% select(outcome) %>% onehot::onehot() %>% predict(dat)  
      lab <- tmp[,1]
    } else {
      lab <- dat[[outcome]]
    }
    
    # Make our DMatrix
    mat <- dat %>% dplyr::select(-outcome) %>% # encode on full boston df
      onehot::onehot() %>% # use onehot to encode variables
      predict(dat) # get OHE matrix

    return(xgb.DMatrix(data = mat, 
                       label = lab))
    
  }
  
}

# helper function
#' @name xg_error
#' @param model xgb object, a fitted boosted model
#' @param test_mat DMatrix, a test set
#' @param metric string (either "r2" or "misclass"), indicates the error metric
#' @returns MSE/misclass rate of the model on the test set
xg_error <- function(model, test_mat, metric = "r2"){
  
  # Get predictions and actual values
  preds = predict(model, test_mat)
  vals = getinfo(test_mat, "label")
  
  if(metric == "r2"){
    
    # Compute R2 if that's what we need
    sse <- sum((vals - preds)^2)
    sst <- sum((vals - mean(vals))^2)
    err <- 1 - sse / sst
    
  } else if(metric == "misclass") {
    
    # Otherwise, get the misclass rate
    err <- mean(preds != vals)
    
  }
  
  return(err)
}
```

Again, we're tuning boosted trees through a validation set approach. Let's tune the learning rate:

```{r}
set.seed(1)

boosted_tree <- news_split %>%
   crossing(learn_rate = 10^seq(-10, -0.1, length.out = 20)) %>% 
   mutate(train_mat = map(train, xgb_matrix, outcome = "shares"), 
          test_mat = map(test, xgb_matrix, outcome = "shares"),
          xg_model = map2(.x = train_mat, .y = learn_rate, 
                          .f = function(x, y) xgb.train(params = list(eta = y,
                                                                      depth = 10, 
                                                                      objective = "reg:squarederror"), 
                                                        data = x, 
                                                        nrounds = 500,
                                                        silent = TRUE)), 
          xg_test_mse = map2(xg_model, test_mat, xg_error, metric = "r2"))
```

Let's check that we're tuning correctly:

```{r}
ggplot(boosted_tree) + 
  geom_line(aes(learn_rate, unlist(xg_test_mse))) +
  labs(x = "Learning Rate", y = "R2")
```

Yes, the test $R^2$ increases to 0 and then decreases further away from 0. This clearly tells us we're overfitting our data and doing a worse job than predicting the mean.

```{r}
boosted_tree %>% 
  arrange(abs(unlist(xg_test_mse))) %>%
  pluck("learn_rate", 1) # 0.0005938602
```

The "best" learning rate is 0.0005938602.

Let's also tune the number of trees fit. It's better to tune all parameters at the same time. This obviously requires more computational capacity. You can do it with map functions, but would need more time and computing power. Here we're taking a "greedy" approach to tuning.

```{r}
set.seed(1)

boosted_tree <- news_split %>%
   crossing(num_trees = seq(from = 600, to = 900, by = 10)) %>% 
   mutate(train_mat = map(train, xgb_matrix, outcome = "shares"), 
          test_mat = map(test, xgb_matrix, outcome = "shares"),
          xg_model = map2(.x = train_mat, .y = num_trees, 
                          .f = function(x, y) xgb.train(params = list(eta = 0.0005938602,
                                                                      depth = 10, 
                                                                      objective = "reg:squarederror"), 
                                                        data = x, 
                                                        nrounds = y,
                                                        silent = TRUE)), 
          xg_test_mse = map2(xg_model, test_mat, xg_error, metric = "r2"))
```

Let's check that we're tunning correctly:

```{r}
ggplot(boosted_tree) + 
  geom_line(aes(num_trees, unlist(xg_test_mse))) +
  labs(x = "Number of trees fit", y = "R2")
```

Again, the test $R^2$ increases to 0 and then decreases further away from 0. This tells us again that we're overfitting our data and doing a worse job than predicting the mean.

```{r}
boosted_tree %>% 
  arrange(abs(unlist(xg_test_mse))) %>%
  pluck("num_trees", 1) # 770
```

The "best" number of trees fit is 770.

You can also tune `gamma`, `max_depth`, `min_child_weight`, and `colsample_bytree` as well. For speed--and because they usually have the largest impact, we'll only tune the learning rate and the number of trees fit.

## Classification

For classification, you have to change `objective` to `"multi:softmax"` and set `num_class` to the number of classes (e.g., if binary, then 2). You'd also have to change to `metric == "misclass"` in the `xg_error` function.

## Variable importance plot

We can also look at variable importance:

```{r}
boosted_tree_best <- boosted_tree %>% 
  arrange(unlist(xg_test_mse)) %>%
  pluck("xg_model", 1)

vip(boosted_tree_best)
```

## Partial dependence plot

You can't use `partial()` to create partial dependence plots for an object of class xgb.Booster. However, you can also fit boosted trees with the package `gbm` and create partial dependence plots with `plot.gbm()`.

# Model validation

## Random forest

Helper function (adjusted from Arend Kuyper's Data Science Manual):

```{r}
# helper function to get misclass rate from a ranger object
#' @name r2_ranger
#' @param model ranger object, a fitted random forest
#' @param test tibble/resample object, a test set
#' @param outcome string, indicates the outcome variable in the data
#' @returns r2 of the model on the test set
r2_ranger <- function(model, test, outcome){
  
  # Check if test is a tibble
  if(!is_tibble(test)){
    test <- test %>% as_tibble()
  }
  
  # Make predictions
  preds <- predict(model, test)$predictions
  
  # Compute R2
  sse <- sum((test[[outcome]] - preds)^2)
  sst <- sum((test[[outcome]] - mean(test[[outcome]]))^2)
  r2 <- 1 - sse / sst
  
  return(r2)
  
}
```

```{r}
random_forest_best_test <- ranger(shares ~ ., 
                      data = news_tune, 
                      mtry = 3,
                      importance = "impurity", 
                      splitrule = "variance")

r2_ranger(random_forest_best_test, news_val, "shares")
```

## Boosted tree

```{r}
boosted_tree_best_test <- xgb.train(params = list(eta = 0.0005938602,
                        depth = 10, 
                        objective = "reg:squarederror"),
          data = xgb_matrix(news_tune, "shares"), 
          nrounds = 770,
          silent = TRUE)

xg_error(boosted_tree_best_test, xgb_matrix(news_val, "shares"), metric = "r2")
```

Neither of them is performing great. We're definitely overfitting our data.

Remember the $R^2s$ from yesterday:

| Model            | Test R2 |
|------------------|---------|
| Kitchen sink OLS | 0.28    |
| Ridge            | 0.21    |
| Lasso            | 0.25    |
| Random forests   | 0.03    |
| Boosted trees    | -0.06   |

Linear models seem to perform best on these data. Keep in mind we're using very little data overall and taking even less advantage given our cross validation strategy. It'd be better to do k-fold cross validation. For instructional purposes (to cover both k-fold cross validation and the validation set approach) and for speed, we're using this strategy.
