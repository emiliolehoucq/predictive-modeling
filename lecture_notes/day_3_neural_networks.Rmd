---
title: "Day 3: Single-layered neural networks"
author: "Emilio Lehoucq"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: true
    toc_float: true
    toc_depth: 3
params:
  notes: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.height = 4.35, fig.width = 4.75, message = FALSE, warning = FALSE)
```

Inspired by neurons in the brain, neural networks map predictors to an output through a series of hidden layers composed of nodes that can be differently activated. Today we'll cover how to implement single-layered neural networks--networks with only one hidden layer. The hyperparameters we can tune are the number of nodes and the weight decay. Single-layered neural networks are useful for problems with low signal to noise ratio (which is the case in analytics problems--think about all the noise in the data we're using!). For problems with more complex signals (e.g., in computer vision), more layers are needed. 

`nnet` only allows for one hidden layer. If you're interested in neural networks with more hidden layers, I recommend Chollet, F. & Allaire, J.J. *Deep Learning with R*, which teaches you the methods and how to implement them with `keras`.

# Packages

These are the packages we're going to use throughout the session:

```{r}
library(tidyverse)
library(janitor) # clean_names
library(robustHD) # standardize
library(nnet) # neural networks
library(caret) # cross validation
library(ALEPlot) # ALE plot
```

# Data

We're going to use the [Online News Popularity Data Set](https://archive.ics.uci.edu/ml/datasets/Online+News+Popularity) from the UCI Machine Learning Repository. The target is the number of shares (`shares`). Here I'm dealing with quick data wrangling to get the data ready for modeling.

Keep in mind it's best to standardize your numerical predictors before fitting neural networks.

```{r}
set.seed(1)

factors <- c("data_channel_is_lifestyle", "data_channel_is_entertainment", "data_channel_is_bus", 
             "data_channel_is_socmed", "data_channel_is_tech", "data_channel_is_world", 
             "weekday_is_monday", "weekday_is_tuesday", "weekday_is_wednesday", 
             "weekday_is_thursday", "weekday_is_friday", "weekday_is_saturday",
             "weekday_is_sunday", "is_weekend") # all factors in the dataset

news <- read_csv(here::here("data/OnlineNewsPopularity.csv")) %>% 
  sample_n(., 1500) %>% 
  clean_names() %>% 
  select(-c(url, lda_00:lda_04)) %>% 
  mutate_each_(funs(factor(.)), factors) %>% 
  mutate_if(is.numeric, funs(standardize(.)))

```

In statistical learning/predictive modeling, you don't care about how your model performs in the data you have at hand, but how it would perform either in the population or in new data that comes your way. Of course, you don't have these data. Cross validation is a set of resampling methods to address this problem. It allows you to estimate your test error. It's important to not use the same set of data for model tuning and for model validation. There are three main types of cross validation: validation set approach, leave one out cross validation, and k-fold cross validation. K-fold cross validation (where usually k=10, 5 or 3, depending on data/computing power) is the ideal. Also, since cross validation involves randomization, it'll give different results across iterations. Ideally, you can do several and average across them.

Today, we're going to use k-fold cross validation for model tuning and the validation set approach for model validation. Now, we're going to split the dataset into two for tuning and validation:

```{r}
set.seed(1)

news_train <- news %>% sample_frac(0.7)
news_test <- news %>% setdiff(news_train)
```

# Single-layered neural networks

Throughout these three days, in addition to the models themselves, we have covered different forms of cross validation and different implementations in R. Particularly, yesterday we covered `map` functions. Today we'll see two other implementations: for loops and `caret`. These tools are useful in statistical learning/predictive modeling beyond the specific models we're covering. In both cases today we'll use 2 replicates of 3-fold cross validation (ideally you can do 5 or 10 folds and more replicates).

## For loops implementation

I like for loops because they are very clear/readable (though they can get messy). However, they can be slow.

First, a helper function to create indices for cross validation:

```{r}
# Adapted from Daniel Apley:

# helper function
#' @name cross_val
#' @param n sample size
#' @param kp number of parts
#' @returns k-length list of indices for each part

cross_val <- function(n, kp) {  
   m <- floor(n/kp) # approximate size of each part
   r <- n-m*kp  
   i <- sample(n,n) # random reordering of the indices
   ind<-list() # creating list of indices for all K parts
   length(ind)<-kp
   
   for (k in 1:kp) {
      if (k <= r) kpart <- ((m+1)*(k-1)+1):((m+1)*k)  
      
         else kpart<-((m+1)*r+m*(k-r-1)+1):((m+1)*r+m*(k-r))
         
      ind[[k]] <- i[kpart]  # indices for kth part of data
   }
   
   ind
   
}
```

```{r}
set.seed(1)

rep <- 2
folds <- 3  
models <- 6
n <- nrow(news_train)
y <- news_train$shares
yhat <- matrix(0, n, models)
mse <- matrix(0, rep, models)

for (j in 1:rep) {
  
  ind <- cross_val(n,folds)
  
  for (k in 1:folds) {
    
     out <- nnet(shares~.,news_train[-ind[[k]],], linout=T, size=1, decay=10, maxit=1000, trace=F)
     yhat[ind[[k]],1] < -as.numeric(predict(out,news_train[ind[[k]],]))
     
     out <- nnet(shares~.,news_train[-ind[[k]],], linout=T, size=1, decay=0.1, maxit=1000, trace=F)
     yhat[ind[[k]],2] <- as.numeric(predict(out,news_train[ind[[k]],]))
  
     out <- nnet(shares~.,news_train[-ind[[k]],], linout=T, size=10, decay=10, maxit=1000, trace=F)
     yhat[ind[[k]],1] <- as.numeric(predict(out,news_train[ind[[k]],]))
     
     out <- nnet(shares~.,news_train[-ind[[k]],], linout=T, size=10, decay=0.1, maxit=1000, trace=F)
     yhat[ind[[k]],2] <- as.numeric(predict(out,news_train[ind[[k]],]))   
     
     out <- nnet(shares~.,news_train[-ind[[k]],], linout=T, size=10, decay=1000, maxit=1000, trace=F)
     yhat[ind[[k]],1] <- as.numeric(predict(out,news_train[ind[[k]],]))
     
     out <- nnet(shares~.,news_train[-ind[[k]],], linout=T, size=10, decay=10, maxit=1000, trace=F)
     yhat[ind[[k]],2] <- as.numeric(predict(out,news_train[ind[[k]],]))  
  } 
  
  mse[j,] <- apply(yhat,2,function(x) sum((y-x)^2))/n
  
} 

ave_mse <- apply(mse,2,mean)

r2 <- 1-ave_mse/var(y)

r2
```

Among these models, the "best" seems to be the one with 1 unit and decay 0.1.

### Classification

For classification, you'd need to switch `linout` in `nnet()` to `FALSE` (which is actually the default). Otherwise the code stays the same.

## `Caret` implementation

For loops are slow and force you to write more code to search over a space of tuning parameters. The implementation in `caret` helps with this:

```{r, echo=TRUE}
set.seed(1)

folds <- createFolds(news_train$shares, k = 3, list = FALSE)

training <- news_train[ folds,]

testing  <- news_train[-folds,]

train_control <- trainControl(method = "repeatedcv",
                           number = 3,
                           repeats = 2)

nn <- train(shares ~ ., data = training, 
            method = "nnet", 
            trControl = train_control,
            verbose = FALSE,
            trace = FALSE,
            tuneGrid = expand.grid(decay = c(0.1, 0.5, 1, 1.5), size = c(1, 5, 10, 15, 17)))
```

Let's check our tuning:

```{r}
trellis.par.set(caretTheme())
ggplot(nn)  
```

We might be tempted to add more units, but they'd be too many for the data we have (there's actually an error message) and the test error is actually stabilizing--not changing meaningfully.

It seems that the "best" single-layered neural network is the one with 17 units and decay parameter of 0.1

# ALE plots

Accumulated local effects plots are an alternative to partial dependence plots (you can read Daniel Apley's article about them, cited in the documentation of `ALEPlot`). You can also use partial dependence plots with `nnet`, though, if you prefer.

Let's do the plot for `num_hrefs`, which was selected as the most important predictor by tree-based methods. 

Neural networks don't do variable selection. One way to get at this is doing several ALE plots (you can use a for loop) and seeing the range of the response over which predictors vary. The larger the range, the most important the predictor is.

```{r, echo=TRUE}
nn_best <- nnet(shares~.,news_train, linout=T, size=17, decay=0.1, maxit=1000, trace=F)

yhatf <- function(X.model, newdata) as.numeric(predict(X.model, newdata))

invisible(ALEPlot(as.data.frame(news_train), nn_best, pred.fun=yhatf, J=7, K=50, NA.plot = TRUE))
rug(pull(news_train))
```

There are barely some non-linearities. Given this, it makes sense that linear methods perform best on these data--while they are less flexible (which doesn't seem to be needed), they are also less complex (and thus prone to overfitting).

# Model validation

A little helper function to get the $R^2$:

```{r}
# helper function
#' @name r2
#' @param model nnet object
#' @param test test data
#' @param outcome string with the response
#' @returns test R2
r2<- function(model, test, outcome){
  
  preds <- predict(model, test)
  
  sse <- sum((test[[outcome]] - preds)^2)
  sst <- sum((test[[outcome]] - mean(test[[outcome]]))^2)
  r2 <- 1 - sse / sst
  
  return(r2)
  
}
```

```{r}
r2(nn_best, news_test, "shares")
```

Terrible! Makes sense given what we discussed yesterday and the ALE plot above.

Remember the $R^2s$ from yesterday:

| Model            | Test R2 |
|------------------|---------|
| Kitchen sink OLS | 0.11    |
| Ridge            | 0.10    |
| Lasso            | 0.10    |
| Random forests   | 0.03    |
| Boosted trees    | -0.06   |
| Neural network   | -1.95   |

Again, linear models perform best on these data. There's no need for more flexibility/complexity.
